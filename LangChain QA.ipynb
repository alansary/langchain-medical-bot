{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow-macos (/Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (0.1.0)\n",
      "Requirement already satisfied: openai in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (0.28.0)\n",
      "Requirement already satisfied: chromadb==0.5.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (0.5.3)\n",
      "Requirement already satisfied: tiktoken in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (0.5.2)\n",
      "Requirement already satisfied: pypdf in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (4.1.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (0.20.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (1.2.1)\n",
      "Requirement already satisfied: requests>=2.28 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (2.28.2)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (1.10.16)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (0.111.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.3) (0.18.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (1.15.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (1.26.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (4.66.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (1.64.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (30.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (3.9.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb==0.5.3) (0.27.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from langchain) (0.0.20)\n",
      "Collecting langchain-core<0.2,>=0.1.7 (from langchain)\n",
      "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
      "  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from build>=1.0.3->chromadb==0.5.3) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from build>=1.0.3->chromadb==0.5.3) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from build>=1.0.3->chromadb==0.5.3) (7.1.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from build>=1.0.3->chromadb==0.5.3) (2.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb==0.5.3) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb==0.5.3) (0.0.4)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb==0.5.3) (3.1.4)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb==0.5.3) (0.0.9)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb==0.5.3) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb==0.5.3) (2.1.1)\n",
      "Requirement already satisfied: anyio in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb==0.5.3) (3.6.1)\n",
      "Requirement already satisfied: certifi in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb==0.5.3) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb==0.5.3) (1.0.4)\n",
      "Requirement already satisfied: idna in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb==0.5.3) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb==0.5.3) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb==0.5.3) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb==0.5.3) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb==0.5.3) (2.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb==0.5.3) (2.29.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb==0.5.3) (1.3.3)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb==0.5.3) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb==0.5.3) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb==0.5.3) (1.26.19)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-core<0.2,>=0.1.7 (from langchain)\n",
      "  Using cached langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
      "  Using cached langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: coloredlogs in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb==0.5.3) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb==0.5.3) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb==0.5.3) (4.25.3)\n",
      "Requirement already satisfied: sympy in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb==0.5.3) (1.12.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-api>=1.2.0->chromadb==0.5.3) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.3) (1.60.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.3) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.3) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.3) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.3) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.3) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.47b0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.3) (0.47b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.3) (70.0.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.3) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.3) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb==0.5.3) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb==0.5.3) (1.11.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from requests>=2.28->chromadb==0.5.3) (3.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from tokenizers>=0.13.2->chromadb==0.5.3) (0.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from typer>=0.9.0->chromadb==0.5.3) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from typer>=0.9.0->chromadb==0.5.3) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from typer>=0.9.0->chromadb==0.5.3) (13.7.1)\n",
      "Requirement already satisfied: httptools>=0.4.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.3) (0.6.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.3) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.3) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.3) (10.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from importlib-resources->chromadb==0.5.3) (3.18.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb==0.5.3) (2.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.3) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.3) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.3) (4.9)\n",
      "Requirement already satisfied: filelock in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.5.3) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.5.3) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb==0.5.3) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.5.3) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.5.3) (2.18.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.5.3) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.5.3) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.5.3) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.3) (0.6.0)\n",
      "Using cached langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
      "Using cached langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of fastjsonschema: [Errno 2] No such file or directory: '/Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages/fastjsonschema-2.16.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow-macos (/Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: langsmith, langchain-core\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.93\n",
      "    Uninstalling langsmith-0.1.93:\n",
      "      Successfully uninstalled langsmith-0.1.93\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.24\n",
      "    Uninstalling langchain-core-0.2.24:\n",
      "      Successfully uninstalled langchain-core-0.2.24\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-chroma 0.1.2 requires langchain-core<0.3,>=0.1.40, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain-openai 0.0.2.post1 requires openai<2.0.0,>=1.6.1, but you have openai 0.28.0 which is incompatible.\n",
      "langchain-text-splitters 0.0.1 requires langchain-core<0.2.0,>=0.1.28, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-core-0.1.23 langsmith-0.0.87\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai chromadb==0.5.3 tiktoken pypdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow-macos (/Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain-chroma in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: chromadb<0.6.0,>=0.4.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from langchain-chroma) (0.5.3)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from langchain-chroma) (0.111.0)\n",
      "Collecting langchain-core<0.3,>=0.1.40 (from langchain-chroma)\n",
      "  Using cached langchain_core-0.2.24-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from langchain-chroma) (1.26.4)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.2.1)\n",
      "Requirement already satisfied: requests>=2.28 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.28.2)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.10.16)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.7.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.18.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.15.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.66.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.64.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (30.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.9.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.27.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.0.4)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (3.1.4)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.0.9)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (2.1.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from langchain-core<0.3,>=0.1.40->langchain-chroma) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.1.40->langchain-chroma)\n",
      "  Using cached langsmith-0.1.93-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from langchain-core<0.3,>=0.1.40->langchain-chroma) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (7.1.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.0.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from email_validator>=2.0.0->fastapi<1,>=0.95.2->langchain-chroma) (2.3.0)\n",
      "Requirement already satisfied: idna>=2.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from email_validator>=2.0.0->fastapi<1,>=0.95.2->langchain-chroma) (3.7)\n",
      "Requirement already satisfied: anyio in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.6.1)\n",
      "Requirement already satisfied: certifi in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.0.4)\n",
      "Requirement already satisfied: sniffio in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from jinja2>=2.11.2->fastapi<1,>=0.95.2->langchain-chroma) (2.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.40->langchain-chroma) (2.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.29.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.3.3)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.26.19)\n",
      "Requirement already satisfied: coloredlogs in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.25.3)\n",
      "Requirement already satisfied: sympy in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.12.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.60.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.47b0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (70.0.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.11.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from requests>=2.28->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (13.7.1)\n",
      "Requirement already satisfied: httptools>=0.4.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.20.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (10.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from importlib-resources->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.18.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.9)\n",
      "Requirement already satisfied: filelock in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2024.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.18.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.6.0)\n",
      "Using cached langchain_core-0.2.24-py3-none-any.whl (377 kB)\n",
      "Using cached langsmith-0.1.93-py3-none-any.whl (139 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of fastjsonschema: [Errno 2] No such file or directory: '/Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages/fastjsonschema-2.16.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow-macos (/Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: langsmith, langchain-core\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.87\n",
      "    Uninstalling langsmith-0.0.87:\n",
      "      Successfully uninstalled langsmith-0.0.87\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.23\n",
      "    Uninstalling langchain-core-0.1.23:\n",
      "      Successfully uninstalled langchain-core-0.1.23\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.0 requires langchain-core<0.2,>=0.1.7, but you have langchain-core 0.2.24 which is incompatible.\n",
      "langchain 0.1.0 requires langsmith<0.1.0,>=0.0.77, but you have langsmith 0.1.93 which is incompatible.\n",
      "langchain-community 0.0.20 requires langchain-core<0.2,>=0.1.21, but you have langchain-core 0.2.24 which is incompatible.\n",
      "langchain-community 0.0.20 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.93 which is incompatible.\n",
      "langchain-openai 0.0.2.post1 requires langchain-core<0.2,>=0.1.7, but you have langchain-core 0.2.24 which is incompatible.\n",
      "langchain-openai 0.0.2.post1 requires openai<2.0.0,>=1.6.1, but you have openai 0.28.0 which is incompatible.\n",
      "langchain-text-splitters 0.0.1 requires langchain-core<0.2.0,>=0.1.28, but you have langchain-core 0.2.24 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-core-0.2.24 langsmith-0.1.93\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages/langchain_community/llms/openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/alansary/miniforge3/envs/main/lib/python3.9/site-packages/langchain_community/llms/openai.py:1070: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"gpt-4o-mini\") # https://platform.openai.com/docs/models/gpt-4o-mini\n",
    "print(llm(\"tell me a joke\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document\n",
    "loader = PyPDFLoader(\"materials/example.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "### For multiple documents \n",
    "# loaders = [....]\n",
    "# documents = []\n",
    "# for loader in loaders:\n",
    "#     documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'materials/example.pdf', 'page': 0}, page_content='Chapter 1 Preview\\n 1\\nArtificial Intelligence\\nIndex Report 2023\\nArtificial Intelligence\\nIndex Report 2023\\nCHAPTER 1:  \\nResearch and \\nDevelopment'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 1}, page_content='Chapter 1 Preview\\n 2\\nArtificial Intelligence\\nIndex Report 2023Overview   3\\nChapter Highlights   4\\n1.1 Publications    5\\nOverview   5\\n Total Number of AI Publications   5\\n By Type of Publication   6\\n By Field of Study   7\\n By Sector   8\\n Cross-Country Collaboration  10\\n Cross-Sector Collaboration  12\\nAI Journal Publications  13\\n Overview  13\\n By Region  14\\n By Geographic Area  15\\n Citations  16\\nAI Conference Publications  17\\n Overview  17\\n By Region  18\\n By Geographic Area  19\\n Citations 20\\nAI Repositories 21\\n Overview 21\\n By Region 22\\n By Geographic Area 23\\n Citations 24\\n Narrative Highlight:   \\nTop Publishing Institutions 25\\n All Fields 25 Computer Vision 27\\n Natural Language Processing 28\\n Speech Recognition 29\\n1.2 Trends in Significant  \\nMachine Learning Systems 30\\nGeneral Machine Learning Systems 30\\n System Types 30\\n Sector Analysis 31\\n National Affiliation 32\\n  Systems 32\\n  Authorship 34\\n Parameter Trends 35\\n Compute Trends 37\\nLarge Language and Multimodal Models 39\\n National Affiliation 39\\n Parameter Count 41\\n Training Compute 42\\n Training Cost 43\\n1.3 AI Conferences  45\\nConference Attendance 45\\n1.4 Open-Source AI Software  47\\nProjects 47\\nStars 49\\nAppendix  50Research and DevelopmentCHAPTER 1 PREVIEW:\\nACCESS THE PUBLIC DATA\\n2'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 2}, page_content='Chapter 1 Preview\\n 3\\nArtificial Intelligence\\nIndex Report 2023Overview\\nThis chapter captures trends in AI R&D. It begins by examining AI publications, \\nincluding journal articles, conference papers, and repositories. Next it considers data \\non significant machine learning systems, including large language and multimodal \\nmodels. Finally, the chapter concludes by looking at AI conference attendance and \\nopen-source AI research. Although the United States and China continue to dominate \\nAI R&D, research efforts are becoming increasingly geographically dispersed.Chapter 1: Research and Development'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 3}, page_content='Chapter 1 Preview\\n 4\\nArtificial Intelligence\\nIndex Report 2023Chapter Highlights\\nThe United States and China  \\nhad the greatest number of  \\ncross-country collaborations in AI \\npublications from 2010 to 2021, \\nalthough the pace of collaboration \\nhas since slowed.  \\nThe number of AI research collaborations between \\nthe United States and China increased roughly 4 \\ntimes since 2010, and was 2.5 times greater than the \\ncollaboration totals of the next nearest country pair, \\nthe United Kingdom and China. However, the total \\nnumber of U.S.-China collaborations only increased \\nby 2.1% from 2020 to 2021, the smallest year-over-\\nyear growth rate since 2010.Industry races ahead  \\nof academia.  \\nUntil 2014, most significant machine \\nlearning models were released by \\nacademia. Since then, industry has taken \\nover. In 2022, there were 32 significant \\nindustry-produced machine learning \\nmodels compared to just three produced \\nby academia. Building state-of-the-art \\nAI systems increasingly requires large \\namounts of data, computer power, and \\nmoney—resources that industry actors \\ninherently possess in greater amounts \\ncompared to nonprofits and academia.\\nAI research is on the rise, across \\nthe board. The total number of AI publications \\nhas more than doubled since 2010. The specific AI \\ntopics that continue to dominate research include \\npattern recognition, machine learning,  \\nand computer vision.\\nChina continues to lead in total \\nAI journal, conference, and \\nrepository publications.  \\nThe United States is still ahead in terms of AI \\nconference and repository citations, but those leads \\nare slowly eroding. Still, the majority of the world’s \\nlarge language and multimodal models (54% in 2022) \\nare produced by American institutions.Large language models \\nare getting bigger and \\nmore expensive.  \\nGPT-2, released in 2019, considered \\nby many to be the first large language \\nmodel, had 1.5 billion parameters and \\ncost an estimated $50,000 USD to \\ntrain. PaLM, one of the flagship large \\nlanguage models launched in 2022, \\nhad 540 billion parameters and cost an \\nestimated $8 million USD—PaLM was \\naround 360 times larger than GPT-2 and \\ncost 160 times more. It’s not just PaLM: \\nAcross the board, large language and \\nmultimodal models are becoming larger \\nand pricier.Chapter 1: Research and Development'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 4}, page_content='Chapter 1 Preview 5\\n496.01\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210100200300400500Number of AI Publications (in Thousands)Number of AI Publications in the World, 2010–21 \\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report Artificial Intelligence\\nIndex Report 2023\\nOverview\\nThe figures below capture the total number \\nof English-language and Chinese-language AI \\npublications globally from 2010 to 2021—by type, \\naffiliation, cross-country collaboration, and cross-\\nindustry collaboration. The section also breaks down 1.1 Publications\\npublication and citation data by region for AI journal \\narticles, conference papers, repositories, and patents.\\nTotal Number of AI Publications\\nFigure 1.1.1 shows the number of AI publications in \\nthe world. From 2010 to 2021, the total number of \\nAI publications more than doubled, growing from \\n200,000 in 2010 to almost 500,000 in 2021.\\n1 See the Appendix for more information on CSET’s methodology. For more on the challenge of defining AI and correctly capturing relevant bibliometric data, see the AI Index team’s \\ndiscussion in the paper “Measurement in AI Policy: Opportunities and Challenges.”This section draws on data from the Center for Security and Emerging Technology (CSET) at Georgetown University. CSET maintains a \\nmerged corpus of scholarly literature that includes Digital Science’s Dimensions, Clarivate’s Web of Science, Microsoft Academic Graph, \\nChina National Knowledge Infrastructure, arXiv, and Papers With Code. In that corpus, CSET applied a classifier to identify English-\\nlanguage publications related to the development or application of AI and ML since 2010. For this year’s report, CSET also used select \\nChinese AI keywords to identify Chinese-language AI papers; CSET did not deploy this method for previous iterations of the AI Index report.1\\nIn last year’s edition of the report, publication trends were reported up to the year 2021. However, given that there is a significant lag in the \\ncollection of publication metadata, and that in some cases it takes until the middle of any given year to fully capture the previous year’s \\npublications, in this year’s report, the AI Index team elected to examine publication trends only through 2021, which we, along with CSET, \\nare confident yields a more fully representative report.1.1 PublicationsChapter 1: Research and Development\\nFigure 1.1.1'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 5}, page_content='Chapter 1 Preview 6\\nArtificial Intelligence\\nIndex Report 2023\\nBy Type of Publication\\nFigure 1.1.2 shows the types of AI publications released \\nglobally over time. In 2021, 60% of all published AI \\ndocuments were journal articles, 17% were conference \\npapers, and 13% were repository submissions. Books, book chapters, theses, and unknown document types \\nmade up the remaining 10% of publications. While \\njournal and repository publications have grown 3 \\nand 26.6 times, respectively, in the past 12 years, the \\nnumber of conference papers has declined since 2019.1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210306090120150180210240270300Number of AI Publications (in Thousands)\\n2.76, Book5.82, Unknown13.77, Book Chapter29.88, Thesis65.21, Repository85.09, Conference293.48, JournalNumber of AI Publications by Type, 2010–21 \\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.2'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 6}, page_content='Chapter 1 Preview 7\\nArtificial Intelligence\\nIndex Report 2023\\nBy Field of Study\\nFigure 1.1.3 shows that publications in pattern \\nrecognition and machine learning have experienced \\nthe sharpest growth in the last half decade. Since \\n2015, the number of pattern recognition papers has roughly doubled while the number of machine learning \\npapers has roughly quadrupled. Following those two \\ntopic areas, in 2021, the next most published AI fields \\nof study were computer vision (30,075), algorithm \\n(21,527), and data mining (19,181).1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210102030405060Number of AI Publications (in Thousands)\\n6.74, Linguistics10.37, Human–Computer Interaction11.57, Control Theory14.99, Natural Language Processing19.18, Data Mining21.53, Algorithm30.07, Computer Vision42.55, Machine Learning59.36, Pattern RecognitionNumber of AI Publications by Field of Study (Excluding Other AI), 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.3'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 7}, page_content='Chapter 1 Preview 8\\nArtificial Intelligence\\nIndex Report 2023\\nBy Sector\\nThis section shows the number of AI publications \\naffiliated with education, government, industry, \\nnonprofit, and other sectors—first globally (Figure \\n1.1.4), then looking at the United States, China, and \\nthe European Union plus the United Kingdom (Figure 1.1.5).2 The education sector dominates in each region. \\nThe level of industry participation is highest in the \\nUnited States, then in the European Union. Since \\n2010, the share of education AI publications has been \\ndropping in each region.1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%80%AI Publications (% of Total)\\n0.22%, Other3.74%, Government7.21%, Industry13.60%, Nonprot75.23%, EducationAI Publications (% of Total) by Sector, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\n2 The categorization is adapted based on the Global Research Identifier Database (GRID). Healthcare, including hospitals and facilities, is included under nonprofit. Publications affiliated with \\nstate-sponsored universities are included in the education sector.Figure 1.1.4'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 8}, page_content='Chapter 1 Preview 9\\nArtificial Intelligence\\nIndex Report 20231.1 PublicationsChapter 1: Research and Development\\n69.17%\\n14.82%\\n12.60%\\n3.21%\\n0.20%69.23%\\n3.92%7.90%18.63%\\n0.33%5.47%77.85%\\n4.74%11.73%\\n0.20%\\n0% 10% 20% 30% 40% 50% 60% 70% 80%OtherGovernmentIndustryNonprotEducation\\nUnited States\\nEuropean Union and United Kingdom\\nChina\\nAI Publications (% of Total)AI Publications (% of Total) by Sector and Geographic Area, 2021\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.5'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 9}, page_content='Chapter 1 Preview 10\\nArtificial Intelligence\\nIndex Report 2023\\nCross-Country Collaboration\\nCross-border collaborations between academics, \\nresearchers, industry experts, and others are a key \\ncomponent of modern STEM (science, technology, \\nengineering, and mathematics) development that \\naccelerate the dissemination of new ideas and the \\ngrowth of research teams. Figures 1.1.6 and 1.1.7 depict \\nthe top cross-country AI collaborations from 2010 \\nto 2021. CSET counted cross-country collaborations \\nas distinct pairs of countries across authors for each \\npublication (e.g., four U.S. and four Chinese-affiliated \\nauthors on a single publication are counted as one \\nU.S.-China collaboration; two publications between \\nthe same authors count as two collaborations).By far, the greatest number of collaborations in the \\npast 12 years took place between the United States \\nand China, increasing roughly four times since 2010. \\nHowever the total number of U.S.-China collaborations \\nonly increased by 2.1% from 2020 to 2021, the smallest \\nyear-over-year growth rate since 2010.\\nThe next largest set of collaborations was between \\nthe United Kingdom and both China and the United \\nStates. In 2021, the number of collaborations between \\nthe United States and China was 2.5 times greater \\nthan between the United Kingdom and China.1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210246810Number of AI Publications (in Thousands)10.47United States and China Collaborations in AI Publications, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.6'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 10}, page_content='Chapter 1 Preview 11\\nArtificial Intelligence\\nIndex Report 20231.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 202101234Number of AI Publications (in Thousands)1.83, United States and France2.61, United States and Australia2.80, China and Australia3.42, United States and Germany4.04, United States and United Kingdom4.13, United Kingdom and ChinaCross-Country Collaborations in AI Publications (Excluding U.S. and China), 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.7'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 11}, page_content='Chapter 1 Preview 12\\nArtificial Intelligence\\nIndex Report 2023\\nCross-Sector Collaboration\\nThe increase in AI research outside of academia has \\nbroadened and grown collaboration across sectors \\nin general. Figure 1.1.8 shows that in 2021 educational \\ninstitutions and nonprofits (32,551) had the greatest \\nnumber of collaborations; followed by industry and educational institutions (12,856); and educational \\nand government institutions (8,913). Collaborations \\nbetween educational institutions and industry have \\nbeen among the fastest growing, increasing 4.2 times \\nsince 2010.1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021051015202530Number of AI Publications (in Thousands)\\n0.63, Industry and Government2.26, Industry and Nonprot2.95, Government and Nonprot8.91, Education and Government12.86, Industry and Education32.55, Education and NonprotCross-Sector Collaborations in AI Publications, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.8'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 12}, page_content='Chapter 1 Preview 13\\nArtificial Intelligence\\nIndex Report 2023\\nAI Journal Publications\\nOverview\\nAfter growing only slightly from 2010 to 2015, the number of AI journal publications grew around 2.3 times since \\n2015. From 2020 to 2021, they increased 14.8% (Figure 1.1.9).1.1 PublicationsChapter 1: Research and Development\\n293.48\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021050100150200250300Number of AI Journal Publications (in Thousands)Number of AI Journal Publications, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.9'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 13}, page_content='Chapter 1 Preview 14\\nArtificial Intelligence\\nIndex Report 20231.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%AI Journal Publications (% of World Total)\\n0.77%, Sub-Saharan Africa2.30%, Rest of the World2.66%, Latin America and the Caribbean4.64%, Middle East and North Africa6.75%, South Asia6.93%, Unknown11.61%, North America17.20%, Europe and Central Asia47.14%, East Asia and PacicAI Journal Publications (% of World Total) by Region, 2010–21 \\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.10By Region3\\nFigure 1.1.10 shows the share of AI journal publications \\nby region between 2010 and 2021. In 2021, East Asia \\nand the Pacific led with 47.1%, followed by Europe \\nand Central Asia (17.2%), and then North America \\n(11.6%). Since 2019, the share of publications from East Asia and the Pacific; Europe and Central Asia; \\nas well as North America have been declining. \\nDuring that period, there has been an increase in \\npublications from other regions such as South Asia; \\nand the Middle East and North Africa.\\n3 Regions in this chapter are classified according to the World Bank analytical grouping.'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 14}, page_content='Chapter 1 Preview 15\\nArtificial Intelligence\\nIndex Report 20231.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%AI Journal Publications (% of World Total)\\n5.56%, India6.88%, Unknown10.03%, United States15.05%, European Union and United Kingdom22.70%, Rest of the World39.78%, ChinaAI Journal Publications (% of World Total) by Geographic Area, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report \\nFigure 1.1.11By Geographic Area4\\nFigure 1.1.11 breaks down the share of AI journal \\npublications over the past 12 years by geographic \\narea. This year’s AI Index included India in recognition \\nof the increasingly important role it plays in the \\nAI ecosystem. China has remained the leader throughout, with 39.8% in 2021, followed by the \\nEuropean Union and the United Kingdom (15.1%), \\nthen the United States (10.0%). The share of Indian \\npublications has been steadily increasing—from 1.3% \\nin 2010 to 5.6% in 2021.\\n4 In this chapter we use “geographic area” based on CSET’s classifications, which are disaggregated not only by country, but also by territory. Further, we count the European Union and the \\nUnited Kingdom as a single geographic area to reflect the regions’ strong history of research collaboration.'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 15}, page_content='Chapter 1 Preview 16\\nArtificial Intelligence\\nIndex Report 20231.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%5%10%15%20%25%30%AI Journal Citations (% of World Total)\\n0.92%, Unknown6.05%, India15.08%, United States21.51%, European Union and United Kingdom27.37%, Rest of the World29.07%, ChinaAI Journal Citations (% of World Total) by Geographic Area, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.12Citations\\nChina’s share of citations in AI journal publications \\nhas gradually increased since 2010, while those of the \\nEuropean Union and the United Kingdom, as well as \\nthose of the United States, have decreased (Figure 1.1.12). China, the European Union and the United \\nKingdom, and the United States accounted for 65.7% \\nof the total citations in the world.'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 16}, page_content='Chapter 1 Preview 17\\nArtificial Intelligence\\nIndex Report 2023\\nAI Conference Publications\\nOverview\\nThe number of AI conference publications peaked in 2019, and fell 20.4% below the peak in 2021 (Figure 1.1.13). \\nThe total number of 2021 AI conference publications, 85,094, was marginally greater than the 2010 total of 75,592.1.1 PublicationsChapter 1: Research and Development\\n85.09\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021020406080100Number of AI Conference Publications (in Thousands)Number of AI Conference Publications, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.13'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 17}, page_content='Chapter 1 Preview 18\\nArtificial Intelligence\\nIndex Report 2023\\nBy Region\\nFigure 1.1.14 shows the number of AI conference \\npublications by region. As with the trend in journal \\npublications, East Asia and the Pacific; Europe \\nand Central Asia; and North America account for \\nthe world’s highest numbers of AI conference \\npublications. Specifically, the share represented by East Asia and the Pacific continues to rise, accounting \\nfor 36.7% in 2021, followed by Europe and Central \\nAsia (22.7%), and then North America (19.6%). The \\npercentage of AI conference publications in South Asia \\nsaw a noticeable rise in the past 12 years, growing from \\n3.6% in 2010 to 8.5% in 2021.1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%5%10%15%20%25%30%35%40%AI Conference Publications (% of World Total)\\n0.60%, Sub-Saharan Africa2.35%, Rest of the World2.76%, Unknown3.07%, Latin America and the Caribbean3.82%, Middle East and North Africa8.45%, South Asia19.56%, North America22.66%, Europe and Central Asia36.72%, East Asia and PacicAI Conference Publications (% of World Total) by Region, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.14'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 18}, page_content='Chapter 1 Preview 19\\nArtificial Intelligence\\nIndex Report 2023\\nBy Geographic Area\\nIn 2021, China produced the greatest share of the \\nworld’s AI conference publications at 26.2%, having \\novertaken the European Union and the United \\nKingdom in 2017. The European Union plus the United \\nKingdom followed at 20.3%, and the United States came in third at 17.2% (Figure 1.1.15). Mirroring trends \\nseen in other parts of the research and development \\nsection, India’s share of AI conference publications is \\nalso increasing.1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%5%10%15%20%25%30%AI Conference Publications (% of World Total)\\n2.70%, Unknown6.79%, India17.23%, United States20.29%, European Union and United Kingdom26.15%, China26.84%, Rest of the WorldAI Conference Publications (% of World Total) by Geographic Area, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.15'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 19}, page_content='Chapter 1 Preview 20\\nArtificial Intelligence\\nIndex Report 2023\\nCitations\\nDespite China producing the most AI conference \\npublications in 2021, Figure 1.1.16 shows that \\nthe United States had the greatest share of AI conference citations, with 23.9%, followed by China’s \\n22.0%. However, the gap between American and \\nChinese AI conference citations is narrowing.1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%5%10%15%20%25%30%35%AI Conference Citations (% of World Total)\\n0.87%, Unknown6.09%, India21.59%, European Union and United Kingdom22.02%, China23.86%, United States25.57%, Rest of the WorldAI Conference Citations (% of World Total) by Geographic Area, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report \\nFigure 1.1.16'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 20}, page_content='Chapter 1 Preview 21\\nArtificial Intelligence\\nIndex Report 2023\\nAI Repositories\\nOverview\\nPublishing pre-peer-reviewed papers on repositories \\nof electronic preprints (such as arXiv and SSRN) \\nhas become a popular way for AI researchers to \\ndisseminate their work outside traditional avenues for \\npublication. These repositories allow researchers to share their findings before submitting them to journals \\nand conferences, thereby accelerating the cycle of \\ninformation discovery. The number of AI repository \\npublications grew almost 27 times in the past 12 years \\n(Figure 1.1.17).1.1 PublicationsChapter 1: Research and Development\\n65.21\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210102030405060Number of AI Repository Publications (in Thousands)Number of AI Repository Publications, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report \\nFigure 1.1.17'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 21}, page_content='Chapter 1 Preview 22\\nArtificial Intelligence\\nIndex Report 2023\\nBy Region\\nFigure 1.1.18 shows that North America has \\nmaintained a steady lead in the world share of AI \\nrepository publications since 2016. Since 2011, the \\nshare of repository publications from Europe and \\nCentral Asia has declined. The share represented by East Asia and the Pacific has grown significantly \\nsince 2010 and continued growing from 2020 to \\n2021, a period in which the year-over-year share of \\nNorth American as well European and Central Asian \\nrepository publications declined.1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%AI Repository Publications (% of World Total)\\n0.34%, Sub-Saharan Africa1.80%, Latin America and the Caribbean1.81%, Rest of the World3.06%, Middle East and North Africa3.41%, South Asia17.88%, East Asia and Pacic21.40%, Europe and Central Asia23.99%, Unknown26.32%, North AmericaAI Repository Publications (% of World Total) by Region, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.18'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 22}, page_content='Chapter 1 Preview 23\\nArtificial Intelligence\\nIndex Report 2023\\nBy Geographic Area\\nWhile the United States has held the lead in the \\npercentage of global AI repository publications since \\n2016, China is catching up, while the European Union \\nplus the United Kingdom’s share continues to drop (Figure 1.1.19). In 2021, the United States accounted \\nfor 23.5% of the world’s AI repository publications, \\nfollowed by the European Union plus the United \\nKingdom (20.5%), and then China (11.9%).1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%AI Repository Publications (% of World Total)\\n2.85%, India11.87%, China18.07%, Rest of the World20.54%, European Union and United Kingdom23.18%, Unknown23.48%, United StatesAI Repository Publications (% of World Total) by Geographic Area, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.19'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 23}, page_content='Chapter 1 Preview 24\\nArtificial Intelligence\\nIndex Report 2023\\nCitations\\nIn the citations of AI repository publications, Figure \\n1.1.20 shows that in 2021 the United States topped \\nthe list with 29.2% of overall citations, maintaining a dominant lead over the European Union plus the \\nUnited Kingdom (21.5%), as well as China (21.0%).1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%AI Repository Citations (% of World Total)\\n1.91%, India4.59%, Unknown20.98%, China21.52%, European Union and United Kingdom21.79%, Rest of the World29.22%, United StatesAI Repository Citations (% of World Total) by Geographic Area, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report \\nFigure 1.1.20'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 24}, page_content='Chapter 1 Preview\\n 25\\nArtificial Intelligence\\nIndex Report 2023Artificial Intelligence\\nIndex Report 2023\\nAll Fields  \\nSince 2010, the institution producing the greatest \\nnumber of total AI papers has been the Chinese \\nAcademy of Sciences (Figure 1.1.21). The next \\ntop four are all Chinese universities: Tsinghua University, the University of the Chinese Academy \\nof Sciences, Shanghai Jiao Tong University, \\nand Zhejiang University.5 The total number of \\npublications released by each of these institutions \\nin 2021 is displayed in Figure 1.1.22.Top Publishing InstitutionsNarrative Highlight: \\n5 It is important to note that many Chinese research institutions are large, centralized organizations with thousands of researchers. It is therefore not entirely surprising that, \\npurely by the metric of publication count, they outpublish most non-Chinese institutions.1.1 PublicationsChapter 1: Research and Development\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 202110987654321Rank1, Chinese Academy of Sciences\\n2, Tsinghua University\\n3, University of Chinese Academy of Sciences\\n4, Shanghai Jiao Tong University\\n5, Zhejiang University\\n6, Harbin Institute of Technology\\n7, Beihang University\\n8, University of Electronic Science and Technology of China\\n9, Peking University\\n10, Massachusetts Institute of TechnologyTop Ten Institutions in the World in 2021 Ranked by Number of AI Publications in All Fields, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.21'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 25}, page_content='Chapter 1 Preview\\n 26\\nArtificial Intelligence\\nIndex Report 2023Artificial Intelligence\\nIndex Report 2023\\nTop Publishing Institutions (cont’d)Narrative Highlight: 1.1 PublicationsChapter 1: Research and Development\\n1,7451,8931,9511,9702,0162,5902,7032,9043,3735,099\\n0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 4,500 5,000Massachusetts Institute of\\nTechnologyPeking UniversityUniversity of Electronic Science\\nand Technology of ChinaBeihang UniversityHarbin Institute of TechnologyZhejiang UniversityShanghai Jiao Tong UniversityUniversity of Chinese Academy\\nof SciencesTsinghua UniversityChinese Academy of Sciences\\nNumber of AI PublicationsTop Ten Institutions in the World by Number of AI Publications in All Fields, 2021\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.22'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 26}, page_content='Chapter 1 Preview 27\\nTop Publishing Institutions (cont’d)Narrative Highlight: Artificial Intelligence\\nIndex Report 2023\\nComputer Vision\\nIn 2021, the top 10 institutions publishing the greatest number of AI computer vision publications were \\nall Chinese (Figure 1.1.23). The Chinese Academy of Sciences published the largest number of such \\npublications, with a total of 562.1.1 PublicationsChapter 1: Research and Development\\n182210229231247289296314316562\\n0 100 200 300 400 500Tianjin UniversityHarbin Institute of TechnologyBeijing Institute of TechnologyWuhan UniversityBeihang UniversityZhejiang UniversityTsinghua UniversityUniversity of Chinese Academy\\nof SciencesShanghai Jiao Tong UniversityChinese Academy of Sciences\\nNumber of AI PublicationsTop Ten Institutions in the World by Number of AI Publications in Computer Vision, 2021\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.23'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 27}, page_content='Chapter 1 Preview 28\\nTop Publishing Institutions (cont’d)Narrative Highlight: Artificial Intelligence\\nIndex Report 2023\\nNatural Language Processing\\nAmerican institutions are represented to a \\ngreater degree in the share of top NLP publishers  \\n(Figure 1.1.24). Although the Chinese Academy of \\nSciences was again the world’s leading institution \\nin 2021 (182 publications), Carnegie Mellon took second place (140 publications), followed by \\nMicrosoft (134). In addition, 2021 was the first year \\nAmazon and Alibaba were represented among the \\ntop-ten largest publishing NLP institutions.1.1 PublicationsChapter 1: Research and Development\\n98100112113116116127134140182\\n0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190Amazon (United States)Alibaba Group (China)University of Chinese Academy\\nof SciencesPeking UniversityGoogle (United States)Carnegie Mellon University\\nAustraliaTsinghua UniversityMicrosoft (United States)Carnegie Mellon UniversityChinese Academy of Sciences\\nNumber of AI PublicationsTop Ten Institutions in the World by Number of AI Publications in Natural Language Processing, 2021\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.24'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 28}, page_content='Chapter 1 Preview 29\\nTop Publishing Institutions (cont’d)Narrative Highlight: Artificial Intelligence\\nIndex Report 2023\\nSpeech Recognition\\nIn 2021, the greatest number of speech recognition papers came from the Chinese Academy of Sciences \\n(107), followed by Microsoft (98) and Google (75) (Figure 1.1.25). The Chinese Academy of Sciences \\nreclaimed the top spot in 2021 from Microsoft, which held first position in 2020.1.1 PublicationsChapter 1: Research and Development\\n545557575961667598107\\n0 10 20 30 40 50 60 70 80 90 100 110Amazon (United States)Chinese University of Hong KongTencent (China)Carnegie Mellon UniversityUniversity of Science\\nand Technology of ChinaTsinghua UniversityUniversity of Chinese Academy\\nof SciencesGoogle (United States)Microsoft (United States)Chinese Academy of Sciences\\nNumber of AI PublicationsTop Ten Institutions in the World by Number of AI Publications in Speech Recognition, 2021\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.25'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 29}, page_content='Chapter 1 Preview 30\\n111223423\\n0 2 4 6 8 10 12 14 16 18 20 22 24GamesOtherText-to-VideoSpeechVisionDrawingMultimodalLanguage\\nNumber of Signicant Machine Learning SystemsNumber of Significant Machine Learning Systems by Domain, 2022\\nSource: Epoch, 2022 | Chart: 2023 AI Index ReportArtificial Intelligence\\nIndex Report 2023\\nGeneral Machine  \\nLearning Systems\\nThe figures below report trends among all machine \\nlearning systems included in the Epoch dataset. For \\nreference, these systems are referred to as significant \\nmachine learning systems throughout the subsection.1.2 Trends in Significant  \\nMachine Learning Systems \\nSystem Types\\nAmong the significant AI machine learning systems \\nreleased in 2022, the most common class of system \\nwas language (Figure 1.2.1). There were 23 significant \\nAI language systems released in 2022, roughly six \\ntimes the number of the next most common system \\ntype, multimodal systems.\\n6 There were 38 total significant AI machine learning systems released in 2022, according to Epoch; however, one of the systems, BaGuaLu, did not have a domain classification \\nand is therefore omitted from Figure 1.2.1.Epoch AI is a collective of researchers investigating and forecasting the development of advanced AI. Epoch curates a database of \\nsignificant AI and machine learning systems that have been released since the 1950s. There are different criteria under which the \\nEpoch team decides to include particular AI systems in their database; for example, the system may have registered a state-of-the-art \\nimprovement, been deemed to have been historically significant, or been highly cited.\\nThis subsection uses the Epoch database to track trends in significant AI and machine learning systems. The latter half of the chapter \\nincludes research done by the AI Index team that reports trends in large language and multimodal models, which are models trained on \\nlarge amounts of data and adaptable to a variety of downstream applications.1.2 Trends in Significant Machine Learning Systems Chapter 1: Research and Development\\nFigure 1.2.16'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 30}, page_content='Chapter 1 Preview 31\\n2002 2004 2006 2008 2010 2012 2014 2016 2018 2020 202205101520253035Number of Signicant Machine Learning Systems\\n0, Nonprot1, Industry-Academia Collaboration2, Research Collective3, Academia32, IndustryNumber of Significant Machine Learning Systems by Sector, 2002–22\\nSource: Epoch, 2022 | Chart: 2023 AI Index ReportArtificial Intelligence\\nIndex Report 2023\\nSector Analysis\\nWhich sector among industry, academia, or nonprofit \\nhas released the greatest number of significant \\nmachine learning systems? Until 2014, most machine \\nlearning systems were released by academia. \\nSince then, industry has taken over (Figure 1.2.2). In \\n2022, there were 32 significant industry-produced machine learning systems compared to just three \\nproduced by academia. Producing state-of-the-art \\nAI systems increasingly requires large amounts of \\ndata, computing power, and money; resources that \\nindustry actors possess in greater amounts compared \\nto nonprofits and academia.Chapter 1: Research and Development\\nFigure 1.2.21.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 31}, page_content='Chapter 1 Preview 32\\nArtificial Intelligence\\nIndex Report 2023\\nNational Affiliation\\nIn order to paint a picture of AI’s evolving \\ngeopolitical landscape, the AI Index research \\nteam identified the nationality of the authors who \\ncontributed to the development of each significant \\nmachine learning system in the Epoch dataset.7\\nSystems\\nFigure 1.2.3 showcases the total number of \\nsignificant machine learning systems attributed to \\nresearchers from particular countries.8 A researcher \\nis considered to have belonged to the country in \\nwhich their institution, for example a university or AI-research firm, was headquartered. In 2022, \\nthe United States produced the greatest number \\nof significant machine learning systems with 16, \\nfollowed by the United Kingdom (8) and China (3). \\nMoreover, since 2002 the United States has outpaced \\nthe United Kingdom and the European Union, as well \\nas China, in terms of the total number of significant \\nmachine learning systems produced (Figure 1.2.4). \\nFigure 1.2.5 displays the total number of significant \\nmachine learning systems produced by country since \\n2002 for the entire world.Chapter 1: Research and Development\\n7 The methodology by which the AI Index identified authors’ nationality is outlined in greater detail in the Appendix.\\n8 A machine learning system is considered to be affiliated with a particular country if at least one author involved in creating the model was affiliated with that country. \\nConsequently, in cases where a system has authors from multiple countries, double counting may occur.11111223816\\n0 2 4 6 8 10 12 14 16SingaporeRussiaIsraelIndiaFranceGermanyCanadaChinaUnited KingdomUnited States\\nNumber of Signicant Machine Learning SystemsNumber of Significant Machine Learning Systems by \\nCountry, 2022\\nSource: Epoch and AI Index, 2022 | Chart: 2023 AI Index Report20022004200620082010201220142016201820202022051015202530Number of  Significant Machine Learning Systems3, China12, European Union and\\nUnited Kingdom16, United StatesNumber of Significant Machine Learning Systems by \\nSelect Geographic Area, 2002–22\\nSource: Epoch and AI Index, 2022 | Chart: 2023 AI Index Report\\nFigure 1.2.3 Figure 1.2.41.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 32}, page_content='Chapter 1 Preview 33\\nArtificial Intelligence\\nIndex Report 2023Chapter 1: Research and Development\\n1–10\\n11–20\\n21–60\\n61–255Number of Machine Learning Systems by Country , 2002–22 (Sum)\\nSource: AI Index, 2022 | Chart: 2023 AI Index Report\\n0\\nFigure 1.2.51.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 33}, page_content='Chapter 1 Preview 34\\nArtificial Intelligence\\nIndex Report 2023\\nAuthorship\\nFigures 1.2.6 to 1.2.8 look at the total number of \\nauthors, disaggregated by national affiliation, that \\ncontributed to the launch of significant machine \\nlearning systems. As was the case with total systems, in 2022 the United States had the greatest number of \\nauthors producing significant machine learning systems, \\nwith 285, more than double that of the United Kingdom \\nand nearly six times that of China (Figure 1.2.6).Chapter 1: Research and Development\\n12378132149139285\\n0 50 100 150 200 250 300FranceIndiaRussiaGermanySwedenIsraelCanadaChinaUnited KingdomUnited States\\nNumber of AuthorsNumber of Authors of Significant Machine Learning \\nSystems by Country, 2022\\nSource: Epoch and AI Index, 2022 | Chart: 2023 AI Index Report20022004200620082010201220142016201820202022050100150200250300350400Number of Authors\\n49, China155, European Union and\\nUnited Kingdom285, United StatesNumber of Authors of Significant Machine Learning \\nSystems by Select Geographic Area, 2002–22\\nSource: Epoch and AI Index, 2022 | Chart: 2023 AI Index Report\\nFigure 1.2.6 Figure 1.2.7\\n1–10\\n11–20\\n21–60\\n61–180\\n181–370\\n371–680\\n681–2000Number of Authors of  Machine Learning Systems by Country , 2002–22 (Sum)\\nSource: AI Index, 2022 | Chart: 2023 AI Index Report\\n0\\nFigure 1.2.81.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 34}, page_content='Chapter 1 Preview 35\\nArtificial Intelligence\\nIndex Report 2023\\nParameter Trends\\nParameters are numerical values that are learned by \\nmachine learning models during training. The value of \\nparameters in machine learning models determines \\nhow a model might interpret input data and make \\npredictions. Adjusting parameters is an essential \\nstep in ensuring that the performance of a machine \\nlearning system is optimized.\\nFigure 1.2.9 highlights the number of parameters of \\nthe machine learning systems included in the Epoch dataset by sector. Over time, there has been a steady \\nincrease in the number of parameters, an increase that \\nhas become particularly sharp since the early 2010s. \\nThe fact that AI systems are rapidly increasing their \\nparameters is reflective of the increased complexity of \\nthe tasks they are being asked to perform, the greater \\navailability of data, advancements in underlying \\nhardware, and most importantly, the demonstrated \\nperformance of larger models.Chapter 1: Research and Development\\n1950 1954 1958 1962 1966 1970 1974 1978 1982 1986 1990 1994 1998 2002 2006 2010 2014 2018 20221.0e+21.0e+41.0e+61.0e+81.0e+101.0e+121.0e+14\\nAcademia Industry Industry-Academia Collaboration Nonprot Research CollectiveNumber of Parameters (Log Scale)Number of Parameters of Significant Machine Learning Systems by Sector, 1950–2022\\nSource: Epoch, 2022 | Chart: 2023 AI Index Report\\nFigure 1.2.91.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 35}, page_content='Chapter 1 Preview 36\\nArtificial Intelligence\\nIndex Report 2023\\nFigure 1.2.10 demonstrates the parameters of machine learning systems by domain. In recent years, there has \\nbeen a rise in parameter-rich systems.Chapter 1: Research and Development\\n1954 1958 1962 1966 1970 1974 1978 1982 1986 1990 1994 1998 2002 2006 2010 2014 2018 20221.0e+21.0e+41.0e+61.0e+81.0e+101.0e+12Language Vision GamesNumber of Parameters (Log Scale)Number of Parameters of Significant Machine Learning Systems by Domain, 1950–2022\\nSource: Epoch, 2022 | Chart: 2023 AI Index Report\\nFigure 1.2.101.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 36}, page_content='Chapter 1 Preview 37\\nArtificial Intelligence\\nIndex Report 2023\\nCompute Trends\\nThe computational power, or “compute,” of AI \\nsystems refers to the amount of computational \\nresources needed to train and run a machine \\nlearning system. Typically, the more complex a \\nsystem is, and the larger the dataset on which it is \\ntrained, the greater the amount of compute required.\\nThe amount of compute used by significant AI machine learning systems has increased exponentially \\nin the last half-decade (Figure 1.2.11).9 The growing \\ndemand for compute in AI carries several important \\nimplications. For example, more compute-intensive \\nmodels tend to have greater environmental impacts, \\nand industrial players tend to have easier access \\nto computational resources than others, such as \\nuniversities.Chapter 1: Research and Development\\n1950 1954 1958 1962 1966 1970 1974 1978 1982 1986 1990 1994 1998 2002 2006 2010 2014 2018 20221.0e+01.0e+31.0e+61.0e+91.0e+121.0e+151.0e+181.0e+211.0e+24Academia Industry Industry-Academia Collaboration Nonprot Research CollectiveTraining Compute (FLOP – Log Scale)Training Compute (FLOP) of Significant Machine Learning Systems by Sector, 1950–2022\\nSource: Epoch, 2022 | Chart: 2023 AI Index Report\\nFigure 1.2.11\\n9 FLOP stands for “Floating Point Operations” and is a measure of the performance of a computational device.1.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 37}, page_content='Chapter 1 Preview 38\\nArtificial Intelligence\\nIndex Report 2023\\nSince 2010, it has increasingly been the case that of all machine learning systems, language models are \\ndemanding the most computational resources.Chapter 1: Research and Development\\n1954 1958 1962 1966 1970 1974 1978 1982 1986 1990 1994 1998 2002 2006 2010 2014 2018 20221.0e+31.0e+61.0e+91.0e+121.0e+151.0e+181.0e+211.0e+24Language Vision GamesTraining Compute (FLOP – Log Scale)Training Compute (FLOP) of Significant Machine Learning Systems by Domain, 1950–2022\\nSource: Epoch, 2022 | Chart: 2023 AI Index Report\\nFigure 1.2.121.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 38}, page_content='Chapter 1 Preview 39\\nArtificial Intelligence\\nIndex Report 2023\\nLarge Language and \\nMultimodal Models\\nLarge language and multimodal models, sometimes \\ncalled foundation models, are an emerging and \\nincreasingly popular type of AI model that is trained \\non huge amounts of data and adaptable to a variety \\nof downstream applications. Large language and \\nmultimodal models like ChatGPT, DALL-E 2, and Make-\\nA-Video have demonstrated impressive capabilities and are starting to be widely deployed in the real world.\\nNational Affiliation\\nThis year the AI Index conducted an analysis of the \\nnational affiliation of the authors responsible for \\nreleasing new large language and multimodal models.10 \\nThe majority of these researchers were from American \\ninstitutions (54.2%) (Figure 1.2.13). In 2022, for the first \\ntime, researchers from Canada, Germany, and India \\ncontributed to the development of large language and \\nmultimodal models.Chapter 1: Research and Development\\n2019 2020 2021 20220%20%40%60%80%100%Authors of Large Language and Multimodal Models (% of Total)\\n0.00%, Korea0.89%, India3.12%, Germany5.80%, Israel6.25%, Canada8.04%, China21.88%, United Kingdom54.02%, United StatesAuthors of Select Large Language and Multimodal Models (% of Total) by Country, 2019–22\\nSource:  Epoch and AI Index, 2022 | Chart: 2023 AI Index Report\\nFigure 1.2.13\\n10 The AI models that were considered to be large language and multimodal models were hand-selected by the AI Index steering committee. It is possible that this selection may have omitted \\ncertain models.Figure 1.2.14 offers a timeline view of the large \\nlanguage and multimodal models that have been \\nreleased since GPT-2, along with the national \\naffiliations of the researchers who produced the \\nmodels. Some of the notable American large \\nlanguage and multimodal models released in \\n2022 included OpenAI’s DALL-E 2 and Google’s PaLM (540B). The only Chinese large language and \\nmultimodal model released in 2022 was GLM-130B, \\nan impressive bilingual (English and Chinese) model \\ncreated by researchers at Tsinghua University. BLOOM, \\nalso launched in late 2022, was listed as indeterminate \\ngiven that it was the result of a collaboration of more \\nthan 1,000 international researchers.1.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 39}, page_content='Chapter 1 Preview 40\\nArtificial Intelligence\\nIndex Report 2023Chapter 1: Research and Development\\n2019-Jan2019-Apr2019-Jul2019-Oct2020-Jan2020-Apr2020-Jul2020-Oct2021-Jan2021-Apr2021-Jul2021-Oct2022-Jan2022-Apr2022-Jul2022-Oct2023-Jan\\nGPT-2Grover-MegaMegatron-LM (Original, 8.3B)T5-3B T5-11BMeenaTuring NLGGPT-3 175B (davinci)ERNIE-GEN (large)DALL-EWu Dao - Wen YuanGPT-NeoPanGu-alpha GPT-J-6BHyperClova CogViewWu Dao 2.0ERNIE 3.0 CodexJurassic-1-JumboMegatron-Turing NLG 530BGopherInstructGPT AlphaCodeGPT-NeoX-20BChinchilla PaLM (540B)DALL·E 2 Stable Diffusion (LDM-KL-8-G)OPT-175B Jurassic-XImagenMinerva (540B)GLM-130BBLOOMSource: AI Index, 2022 | Chart: 2023 AI Index Report\\nUnited States\\nUnited Kingdom\\nChina\\nUnited States,\\nUnited Kingdom,\\nGermany, India\\nKoreaCanada\\nIsrael\\nGermany\\nIndeterminateTimeline and National Affiliation of Select Large Language and Multimodal Model Releases\\nFigure 1.2.1411\\n11 While we were conducting the analysis to produce Figure 1.2.14, Irene Solaiman published a paper  that has a similar analysis. We were not aware of the paper at the time of our research.1.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 40}, page_content='Chapter 1 Preview 41\\nArtificial Intelligence\\nIndex Report 2023\\nParameter Count\\nOver time, the number of parameters of newly released \\nlarge language and multimodal models has massively \\nincreased. For example, GPT-2, which was the first \\nlarge language and multimodal model released in 2019, \\nonly had 1.5 billion parameters. PaLM, launched by Google in 2022, had 540 billion, nearly 360 times \\nmore than GPT-2. The median number of parameters \\nin large language and multimodal models is increasing \\nexponentially over time (Figure 1.2.15).Chapter 1: Research and Development\\nGPT-2\\nGrover-MegaMegatron-LM (Original, 8.3B)\\nT5-3BT5-11B\\nMeenaTuring NLGGPT-3 175B (davinci)\\nERNIE-GEN (large)DALL-E\\nWu Dao - Wen Yuan\\nGPT-NeoPanGu-α\\nGPT-J-6BHyperClova\\nCogVie wWu Dao 2.0\\nERNIE 3.0CodexJurassic-1-JumboMegatron-Turing NLG 530B\\nGopher\\nGPT-NeoX-20BChinchillaPaLM (540B)\\nDALL·E 2\\nStable Diusion (LDM-KL-8-G)OPT-175B\\nJurassic-XMinerva (540B)\\nGLM-130BBLOOM\\n2019-Feb 2019-May 2019-Sep2019-Oct 2020-Jan2020-Feb 2020-May 2020-Aug 2021-Jan 2021-Mar2021-Apr2021-May2021-Jun2021-Jul2021-Aug 2021-Oct 2021-Dec 2022-Feb2022-Mar2022-Apr2022-May2022-Jun 2022-Aug 2022-Nov3.2e+81.0e+93.2e+91.0e+103.2e+101.0e+113.2e+111.0e+123.2e+12Number of Parameters (Log Scale)Number of Parameters of Select Large Language and Multimodal Models, 2019–22\\nSource: Epoch, 2022 | Chart: 2023 AI Index Report\\nFigure 1.2.151.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 41}, page_content='Chapter 1 Preview 42\\nArtificial Intelligence\\nIndex Report 2023\\nTraining Compute\\nThe training compute of large language and multimodal \\nmodels has also steadily increased (Figure 1.2.16). The \\ncompute used to train Minerva (540B), a large language \\nand multimodal model released by Google in June \\n2022 that displayed impressive abilities on quantitative reasoning problems, was roughly nine times greater \\nthan that used for OpenAI’s GPT-3, which was \\nreleased in June 2022, and roughly 1839 times greater \\nthan that used for GPT-2 (released February 2019).Chapter 1: Research and Development\\nGPT-2Megatron-LM (Original, 8.3B)T5-3BT5-11BMeena\\nTuring NLGGPT-3 175B (davinci)\\nDALL-E\\nWu Dao - Wen YuanGPT-NeoPanGu-α\\nGPT-J-6BHyperClova\\nCogView\\nERNIE 3.0Jurassic-1-JumboMegatron-Turing NLG 530B\\nGopher\\nAlphaCodePaLM (540B)\\nChinchillaOPT-175BMinerva (540B)\\nGLM-130BBLOOM\\n2019-F\\neb2019-\\nSep2019-Oct 2020-Jan2020-F\\neb2020-M\\nay202\\n1-Jan202\\n1-M\\nar202\\n1-Apr2021-M\\nay202\\n1-Jul202\\n1-Au\\ng202\\n1-Oct202\\n1-Dec2022-F\\neb2022-M\\nar2022-\\nApr2022-M\\nay2022-Jun 2022-\\nAug2022-No\\nv1.0e+183.2e+181.0e+193.2e+191.0e+203.2e+201.0e+213.2e+211.0e+223.2e+221.0e+233.2e+231.0e+243.2e+24Training Compute (FLOP – Log Scale)Training Compute (FLOP) of Select Large Language and Multimodal Models, 2019–22\\nSource: Epoch, 2022 | Chart: 2023 AI Index Report\\nStable Diffusion GPT-NeoX-20B\\nFigure 1.2.161.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 42}, page_content='Chapter 1 Preview 43\\nArtificial Intelligence\\nIndex Report 2023\\nTraining Cost\\nA particular theme of the discourse around large \\nlanguage and multimodal models has to do with their \\nhypothesized costs. Although AI companies rarely speak \\nopenly about training costs, it is widely speculated that \\nthese models cost millions of dollars to train and will \\nbecome increasingly expensive with scale.\\nThis subsection presents novel analysis in which the \\nAI Index research team generated estimates for the \\ntraining costs of various large language and multimodal \\nmodels (Figure 1.2.17). These estimates are based on the \\nhardware and training time disclosed by the models’ \\nauthors. In cases where training time was not disclosed, \\nwe calculated from hardware speed, training compute, \\nand hardware utilization efficiency. Given the possible \\nvariability of the estimates, we have qualified each estimate with the tag of mid, high, or low: mid where \\nthe estimate is thought to be a mid-level estimate, \\nhigh where it is thought to be an overestimate, and \\nlow where it is thought to be an underestimate. In \\ncertain cases, there was not enough data to estimate \\nthe training cost of particular large language and \\nmultimodal models, therefore these models were \\nomitted from our analysis.\\nThe AI Index estimates validate popular  claims that \\nlarge language and multimodal models are increasingly \\ncosting millions of dollars to train. For example, \\nChinchilla, a large language model launched by \\nDeepMind in May 2022, is estimated to have cost $2.1 \\nmillion, while BLOOM’s training is thought to have cost \\n$2.3 million.Chapter 1: Research and Development\\nFigure 1.2.170.051.971.47\\n0.111.80\\n0.230.020.090.43 0.270.01 0.1411.35\\n8.55\\n0.09 0.242.118.01\\n0.601.69\\n1.03\\n0.162.29GPT-2\\nT5-11B\\nMeena\\nTuring NLG\\nGPT-3 175B\\nDALL-E\\nWu Dao - Wen Yuan\\nGPT-Neo\\nGPT-J-6B\\nHyperClova\\nERNIE 3.0\\nCodex\\nMegatron-Turing NLG 530B\\nGopher\\nAlphaCode\\nGPT-NeoX-20B\\nChinchilla\\nPaLM (540B)\\nStable Diusion (LDM-KL-8-G)\\nOPT-175B\\nMinerva (540B)\\nGLM-130B\\nBLOOM\\n2019 2020 2021 2022024681012Mid High LowEstimated Training Cost of Select Large Language and Multimodal Models\\nSource: AI Index, 2022 | Chart: 2023 AI Index Report\\nTraining Cost\\n(in Millions of U.S. Dollars)\\n12 See Appendix for the complete methodology behind the cost estimates.1.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 43}, page_content='Chapter 1 Preview 44\\nArtificial Intelligence\\nIndex Report 2023\\nThere is also a clear relationship between the cost of large language and multimodal models and their size. \\nAs evidenced in Figures 1.2.18 and 1.2.19, the large language and multimodal models with a greater number of \\nparameters and that train using larger amounts of compute tend to be more expensive.Chapter 1: Research and Development\\nFigure 1.2.18 Figure 1.2.19BLOOMGLM-130BMinerva (540B)\\nOPT-175B\\nStable DiusionPaLM (540B)\\nChinchilla\\nGPT-NeoX-20BAlphaCodeGopherMegatron-Turing NLG 530B\\nCodexERNIE 3.0HyperClova\\nGPT-J-6B\\nGPT-NeoWu Dao - Wen YuanDALL-EGPT-3 175B\\nTuring NLG\\nMeenaT5-11B\\nGPT-2\\n10k 100k 1M 10M1.0e+92.0e+95.0e+91.0e+102.0e+105.0e+101.0e+112.0e+115.0e+11\\nTraining Cost (in U.S. Dollars - Log Scale)Number of Parameters (Log Scale)Estimated Training Cost of Select Large Language\\nand Multimodal Models and Number of Parameters\\nSource: AI Index, 2022 | Chart: 2023 AI Index Report\\nBLOOM\\nGLM-130BMinerva (540B)\\nOPT-175B\\nChinchilla\\nGPT-NeoX-20B\\nAlphaCodeGopherPaLM (540B)\\nMegatron-Turing NLG 530B\\nERNIE 3.0Stable Diffusion\\nGPT-J-6B\\nGPT-Neo\\nWu Dao - Wen YuanDALL-E\\nTuring NLGMeena\\nT5-11B\\nGPT-2\\n10k 100k 1M 10M1.0e+181.0e+201.0e+221.0e+24\\nTraining Cost (in U.S. Dollars - Log Scale)Training Compute (FLOP – Log Scale)Estimated Training Cost of Select Large Language and \\nMultimodal Models and Training Compute (FLOP)\\nSource: AI Index, 2022 | Chart: 2023 AI Index Report1.2 Trends in Significant Machine Learning Systems '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 44}, page_content='Chapter 1 Preview 45\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 20220102030405060708090Number of Attendees (in Thousands)59.45Number of Attendees at Select AI Conferences, 2010–22 \\nSource: AI Index, 2022 | Chart: 2023 AI Index ReportArtificial Intelligence\\nIndex Report 2023\\nConference Attendance\\nAfter a period of increasing attendance, the total \\nattendance at the conferences for which the AI \\nIndex collected data dipped in 2021 and again in \\n2022 (Figure 1.3.1).13 This decline may be attributed \\nto the fact that many conferences returned to hybrid \\nor in-person formats after being fully virtual in \\n2020 and 2021. For example, the International Joint \\nConference on Artificial Intelligence (IJCAI) and the 1.3 AI Conferences\\nInternational Conference on Principles of Knowledge \\nRepresentation and Reasoning (KR) were both held \\nstrictly in-person.\\nNeural Information Processing Systems (NeurIPS) \\ncontinued to be one of the most attended \\nconferences, with around 15,530 attendees (Figure \\n1.3.2).14 The conference with the greatest one-\\nyear increase in attendance was the International \\nConference on Robotics and Automation (ICRA), \\nfrom 1,000 in 2021 to 8,008 in 2022.\\n13 This data should be interpreted with caution given that many conferences in the last few years have had virtual or hybrid formats. Conference organizers report that \\nmeasuring the exact attendance numbers at virtual conferences is difficult, as virtual conferences allow for higher attendance of researchers from around the world.\\n14 In 2021, 9,560 of the attendees attended NeurIPS in-person and 5,970 remotely.AI conferences are key venues for researchers to share their work and connect with peers and collaborators. Conference attendance is an \\nindication of broader industrial and academic interest in a scientific field. In the past 20 years, AI conferences have grown in size, number, \\nand prestige. This section presents data on the trends in attendance at major AI conferences.1.3 AI ConferencesChapter 1: Research and Development\\nFigure 1.3.1'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 45}, page_content='Chapter 1 Preview 46\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022051015202530Number of Attendees (in Thousands)\\n3.56, AAAI4.32, IROS5.35, ICLR7.73, ICML8.01, ICRA10.17, CVPR15.53, NeurIPSAttendance at Large Conferences, 2010–22\\nSource: AI Index, 2022 | Chart: 2023 AI Index Report\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 20220.000.501.001.502.002.503.003.50Number of Attendees (in Thousands)\\n0.12, KR0.39, ICAPS0.50, AAMAS0.66, UAI1.09, FaccT2.01, IJCAIAttendance at Small Conferences, 2010–22\\nSource: AI Index, 2022 | Chart: 2023 AI Index ReportArtificial Intelligence\\nIndex Report 20231.3 AI ConferencesChapter 1: Research and Development\\nFigure 1.3.2\\nFigure 1.3.3'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 46}, page_content='Chapter 1 Preview 47\\n2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022050100150200250300350Number of AI Projects (in Thousands)348Number of GitHub AI Projects, 2011–22\\nSource: GitHub, 2022; OECD.AI, 2022 | Chart: 2023 AI Index Report Artificial Intelligence\\nIndex Report 2023\\nProjects\\nA GitHub project is a collection of files that \\ncan include the source code, documentation, \\nconfiguration files, and images that constitute a 1.4 Open-Source AI Software\\nsoftware project. Since 2011, the total number of \\nAI-related GitHub projects has steadily increased, \\ngrowing from 1,536 in 2011 to 347,934 in 2022.GitHub is a web-based platform where individuals and coding teams can host, review, and collaborate on various code repositories. \\nGitHub is used extensively by software developers to manage and share code, collaborate on various projects, and support open-source \\nsoftware. This subsection uses data provided by GitHub and the OECD.AI policy observatory. These trends can serve as a proxy for some \\nof the broader trends occuring in the world of open-source AI software not captured by academic publication data.1.4 Open-Source AI SoftwareChapter 1: Research and Development\\nFigure 1.4.1'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 47}, page_content='Chapter 1 Preview 48\\n2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 20220%5%10%15%20%25%30%35%40%AI Projects (% of Total)\\n2.40%, China14.00%, United States17.30%, European Union and United Kingdom24.19%, India42.11%, Rest of the WorldGitHub AI Projects (% Total) by Geographic Area, 2011–22\\nSource: GitHub, 2022; OECD.AI, 2022 | Chart: 2023 AI Index Report Artificial Intelligence\\nIndex Report 2023\\nAs of 2022, a large proportion of GitHub AI projects \\nwere contributed by software developers in India \\n(24.2%) (Figure 1.4.2). The next most represented \\ngeographic area was the European Union and the United Kingdom (17.3%), and then the United States \\n(14.0%). The share of American GitHub AI projects \\nhas been declining steadily since 2016.1.4 Open-Source AI SoftwareChapter 1: Research and Development\\nFigure 1.4.2'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 48}, page_content='Chapter 1 Preview 49\\n2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 20220.000.501.001.502.002.503.003.50Number of Cumulative GitHub Stars (in Millions) 0.46, India1.53, China2.34, European Union and United Kingdom2.69, Rest of the World3.44, United StatesNumber of GitHub Stars by Geographic Area, 2011–22\\nSource: GitHub, 2022; OECD.AI, 2022 | Chart: 2023 AI Index Report Artificial Intelligence\\nIndex Report 2023\\nStars\\nGitHub users can bookmark or save a repository \\nof interest by “starring” it. A GitHub star is similar \\nto a “like” on a social media platform and indicates \\nsupport for a particular open-source project. Some of \\nthe most starred GitHub repositories include libraries \\nlike TensorFlow, OpenCV, Keras, and PyTorch, which \\nare widely used by software developers in the AI \\ncoding community.Figure 1.4.3 shows the cumulative number of stars \\nattributed to projects belonging to owners of various \\ngeographic areas. As of 2022, GitHub AI projects \\nfrom the United States received the most stars, \\nfollowed by the European Union and the United \\nKingdom, and then China. In many geographic areas, \\nthe total number of new GitHub stars has leveled off \\nin the last few years.1.4 Open-Source AI SoftwareChapter 1: Research and Development\\nFigure 1.4.3'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 49}, page_content='50\\nArtificial Intelligence\\nIndex Report 2023 Chapter 1 PreviewCenter for Security and \\nEmerging Technology, \\nGeorgetown University\\nPrepared by Sara Abdulla and James Dunham\\nThe Center for Security and Emerging Technology \\n(CSET) is a policy research organization within \\nGeorgetown University’s Walsh School of Foreign \\nService that produces data-driven research at the \\nintersection of security and technology, providing \\nnonpartisan analysis to the policy community.\\nFor more information about how CSET analyzes \\nbibliometric and patent data, see the Country Activity \\nTracker (CAT) documentation on the Emerging \\nTechnology Observatory’s website.1 Using CAT, users \\ncan also interact with country bibliometric, patent, \\nand investment data.2\\nPublications from CSET Merged Corpus of \\nScholarly Literature\\nSource\\nCSET’s merged corpus of scholarly literature \\ncombines distinct publications from Digital Science’s \\nDimensions, Clarivate’s Web of Science, Microsoft \\nAcademic Graph, China National Knowledge \\nInfrastructure, arXiv, and Papers With Code.3Methodology\\nTo create the merged corpus, CSET deduplicated \\nacross the listed sources using publication metadata, \\nand then combined the metadata for linked \\npublications. To identify AI publications, CSET used an \\nEnglish-language subset of this corpus: publications \\nsince 2010 that appear AI-relevant.4 CSET researchers \\ndeveloped a classifier for identifying AI-related \\npublications by leveraging the arXiv repository, where \\nauthors and editors tag papers by subject. Additionally, \\nCSET uses select Chinese AI keywords to identify \\nChinese-language AI papers.5\\nTo provide a publication’s field of study, CSET \\nmatches each publication in the analytic corpus \\nwith predictions from Microsoft Academic Graph’s \\nfield-of-study model, which yields hierarchical labels \\ndescribing the published research field(s) of study and \\ncorresponding scores.6 CSET researchers identified \\nthe most common fields of study in our corpus of \\nAI-relevant publications since 2010 and recorded \\npublications in all other fields as “Other AI.” English-\\nlanguage AI-relevant publications were then tallied by \\ntheir top-scoring field and publication year.\\nCSET also provided year-by-year citations for AI-\\nrelevant work associated with each country. A \\npublication is associated with a country if it has at AppendixChapter 1: Research and DevelopmentAppendix\\n1 https:/ /eto.tech/tool-docs/cat/\\n2 https:/ /cat.eto.tech/\\n3 All CNKI content is furnished by East View Information Services, Minneapolis, Minnesota, USA.\\n4 For more information, see James Dunham, Jennifer Melot, and Dewey Murdick, “Identifying the Development and Application of Artificial Intelligence in Scientific Text,” arXiv [cs.DL], \\nMay 28, 2020, https:/ /arxiv.org/abs/2002.07143 .\\n5 This method was not used in CSET’s data analysis for the 2022 HAI Index report.\\n6 These scores are based on cosine similarities between field-of-study and paper embeddings. See Zhihong Shen, Hao Ma, and Kuansan Wang, “A Web-Scale System for Scientific \\nKnowledge Exploration,” arXiv [cs.CL], May 30, 2018, https:/ /arxiv.org/abs/1805.12216 .'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 50}, page_content='51\\nArtificial Intelligence\\nIndex Report 2023 Chapter 1 Previewleast one author whose organizational affiliation(s) \\nare located in that country. Citation counts aren’t \\navailable for all publications; those without counts \\nweren’t included in the citation analysis. Over 70% of \\nEnglish-language AI papers published between 2010 \\nand 2020 have citation data available.\\nCSET counted cross-country collaborations as \\ndistinct pairs of countries across authors for each \\npublication. Collaborations are only counted once: \\nFor example, if a publication has two authors from \\nthe United States and two authors from China, it is \\ncounted as a single United States-China collaboration.\\nAdditionally, publication counts by year and by \\npublication type (e.g., academic journal articles, \\nconference papers) were provided where available. \\nThese publication types were disaggregated by \\naffiliation country as described above.\\nCSET also provided publication affiliation sector(s) \\nwhere, as in the country attribution analysis, sectors \\nwere associated with publications through authors’ \\naffiliations. Not all affiliations were characterized in \\nterms of sectors; CSET researchers relied primarily \\non GRID from Digital Science for this purpose, and \\nnot all organizations can be found in or linked to \\nGRID.7 Where the affiliation sector is available, papers \\nwere counted toward these sectors, by year. Cross-\\nsector collaborations on academic publications \\nwere calculated using the same method as in the \\ncross-country collaborations analysis. We use HAI’s \\nstandard regions mapping for geographic analysis, \\nand the same principles for double-counting apply for \\nregions as they do for countries.Epoch National  \\nAffiliation Analysis\\nThe AI forecasting research group Epoch maintains \\na dataset of landmark AI and ML models, along with \\naccompanying information about their creators and \\npublications, such as the list of their (co)authors, number \\nof citations, type of AI task accomplished, and amount \\nof compute used in training.\\nThe nationalities of the authors of these papers have \\nimportant implications for geopolitical AI forecasting. \\nAs various research institutions and technology \\ncompanies start producing advanced ML models, the \\nglobal distribution of future AI development may shift \\nor concentrate in certain places, which in turn affects \\nthe geopolitical landscape because AI is expected to \\nbecome a crucial component of economic and military \\npower in the near future.\\nTo track the distribution of AI research contributions on \\nlandmark publications by country, the Epoch dataset is \\ncoded according to the following methodology:\\n 1.  A snapshot of the dataset was taken on \\nNovember 14, 2022. This includes papers about \\nlandmark models, selected using the inclusion \\ncriteria of importance, relevance, and uniqueness, \\nas described in the Compute Trends dataset \\ndocumentation.8\\n 2.  The authors are attributed to countries based \\non their affiliation credited on the paper. For \\ninternational organizations, authors are attributed \\nto the country where the organization is \\nheadquartered, unless a more specific location \\nis indicated. The number of authors from each \\ncountry represented are added up and recorded. \\n7 See https:/ /www.grid.ac/ for more information about the GRID dataset from Digital Science.\\n8 https:/ /epochai.org/blog/compute-trends; see note on “milestone systems.”Chapter 1: Research and DevelopmentAppendix'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 51}, page_content='52\\nArtificial Intelligence\\nIndex Report 2023 Chapter 1 PreviewIf an author has multiple affiliations in different \\ncountries, they are split between those \\ncountries proportionately.9\\n 3.  Each paper in the dataset is normalized to \\nequal value by dividing the counts on each \\npaper from each country by the total number of \\nauthors on that paper.10\\n 4.  All of the landmark publications are aggregated \\nwithin time periods (e.g., monthly or yearly) with \\nthe normalized national contributions added up \\nto determine what each country’s contribution \\nto landmark AI research was during each time \\nperiod.\\n 5.  The contributions of different countries are \\ncompared over time to identify any trends.\\nLarge Language and \\nMultimodal Models\\nThe following models were identified by members \\nof the AI Index Steering Committee as the large \\nlanguage and multimodal models that would be \\nincluded as part of the large language and multimodal \\nmodel analysis:Chapter 1: Research and DevelopmentAppendix\\nAlphaCode\\nBLOOM\\nChinchilla\\nCodex\\nCogView\\nDALL-E\\nDALL-E 2\\nERNIE 3.0\\nERNIE-GEN (large)GLM-130B\\nGopher\\nGPT-2\\nGPT-3 175B (davinci)\\nGPT-J-6B\\nGPT-Neo\\nGPT-NeoX-20B\\nGrover-Mega\\nHyperCLOVAImagen\\nInstructGPT\\nJurassic-1-Jumbo\\nJurassic-X\\nMeena\\nMegatron-LM (original, \\n8.3B)\\nMegatron-Turing NLG \\n530B\\nMinerva (540B)OPT-175B\\nPaLM (540B)\\nPanGu-alpha\\nStable Diffusion (LDM-\\nKL-8-G)\\nT5-3B\\nT5-11B\\nTuring NLG\\nWu Dao 2.0\\nWu Dao – Wen Yuan\\n9 For example, an author employed by both a Chinese university and a Canadian technology firm would be counted as 0.5 researchers from China and 0.5 from Canada.\\n10 This choice is arbitrary. Other plausible alternatives include weighting papers by their number of citations, or assigning greater weight to papers with more authors.\\n11 Hardware utilization rates: Every paper that reported the hardware utilization efficiency during training provided values between 30% and 50%. The AI Index used the reported numbers \\nwhen available, or used 40% when values were not provided.Large Language and \\nMultimodal Models Training \\nCost Analysis\\nCost estimates for the models were based directly \\non the hardware and training time if these were \\ndisclosed by the authors; otherwise, the AI Index \\ncalculated training time from the hardware speed, \\ntraining compute, and hardware utilization efficiency.11 \\nTraining time was then multiplied by the closest cost \\nrate for the hardware the AI Index could find for the \\norganization that trained the model. If price quotes \\nwere available before and after the model’s training, \\nthe AI Index interpolated the hardware’s cost rate \\nalong an exponential decay curve.\\nThe AI Index classified training cost estimates as \\nhigh, middle, or low. The AI Index called an estimate \\nhigh if it was an upper bound or if the true cost was \\nmore likely to be lower than higher: For example, \\nPaLM was trained on TPU v4 chips, and the AI Index \\nestimated the cost to train the model on these chips \\nfrom Google’s public cloud compute prices, but the '),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 52}, page_content='53\\nArtificial Intelligence\\nIndex Report 2023 Chapter 1 PreviewAI Conferences \\nThe AI Index reached out to the organizers of various \\nAI conferences in 2022 and asked them to provide \\ninformation on total attendance. Some conferences \\nposted their attendance totals online; when this was \\nthe case, the AI Index used those reported totals and \\ndid not reach out to the conference organizers.\\nGitHub\\nThe GitHub data was provided to the AI Index \\nthrough OECD.AI, an organization with whom \\nGitHub partners that provides data on open-\\nsource AI software. The AI Index reproduces the \\nmethodological note that is included by OECD.AI on \\nits website, for the GitHub Data.\\nBackground\\nSince its creation in 2007, GitHub has become \\nthe main provider of internet hosting for software \\ndevelopment and version control. Many technology \\norganizations and software developers use GitHub \\nas a primary place for collaboration. To enable \\ncollaboration, GitHub is structured into projects, or \\n“repositories,” which contain a project’s files and each file’s revision history. The analysis of GitHub \\ndata could shed light on relevant metrics about who \\nis developing AI software, where, and how fast, and \\nwho is using which development tools. These metrics \\ncould serve as proxies for broader trends in the field \\nof software development and innovation.\\nIdentifying AI Projects\\nArguably, a significant portion of AI software \\ndevelopment takes place on GitHub. OECD.AI  \\npartners with GitHub to identify public AI projects—\\nor “repositories”—following the methodology \\ndeveloped by Gonzalez et al.,2020. Using the 439 \\ntopic labels identified by Gonzalez et al.—as well as \\nthe topics “machine learning,” “deep learning,” and \\n“artificial intelligence”—GitHub provides OECD.\\nAI with a list of public projects containing AI code. \\nGitHub updates the list of public AI projects on a \\nquarterly basis, which allows OECD.AI to capture \\ntrends in AI software development over time.\\nObtaining AI Projects’ Metadata\\nOECD.AI uses GitHub’s list of public AI projects \\nto query GitHub’s public API and obtain more \\ninformation about these projects. Project metadata \\nmay include the individual or organization that \\ncreated the project; the programming language(s) \\n(e.g., Python) and development tool(s) (e.g., Jupyter \\nNotebooks) used in the project; as well as information \\nabout the contributions—or “commits”—made to it, \\nwhich include the commit’s author and a timestamp. \\nIn practical terms, a contribution or “commit” is an \\nindividual change to a file or set of files. Additionally, \\nGitHub automatically suggests topical tags to each \\nproject based on its content. These topical tags need \\nto be confirmed or modified by the project owner(s) \\nto appear in the metadata.Chapter 1: Research and DevelopmentAppendix\\ninternal cost to Google is probably lower than what \\nthey charge others to rent their hardware. The AI \\nIndex called an estimate low if it was a lower bound \\nor if the true cost was likely higher: For example, \\nERNIE was trained on NVIDIA Tesla v100 chips and \\npublished in July 2021; the chips cost $0.55 per hour \\nin January 2023, so the AI Index could get a low \\nestimate of the cost using this rate, but the training \\nhardware was probably more expensive two years \\nearlier. Middle estimates are a best guess, or those \\nthat equally well might be lower or higher.'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 53}, page_content='54\\nArtificial Intelligence\\nIndex Report 2023 Chapter 1 PreviewMapping Contributions to AI Projects to a \\nCountry\\nContributions to public AI projects are mapped \\nto a country based on location information at the \\ncontributor level and at the project level.\\na) Location information at the contributor level:\\n •  GitHub’s “Location” field: Contributors can \\nprovide their location in their GitHub account. \\nGiven that GitHub’s location field accepts free \\ntext, the location provided by contributors is \\nnot standardized and could belong to different \\nlevels (e.g., suburban, urban, regional, or \\nnational). To allow cross-country comparisons, \\nMapbox is used to standardize all available \\nlocations to the country level.\\n •   Top level domain: Where the location field \\nis empty or the location is not recognized, a \\ncontributor’s location is assigned based on his \\nor her email domain (e.g., .fr, .us, etc.).\\nb) Location information at the project level:\\n •  Project information: Where no location \\ninformation is available at the contributor \\nlevel, information at the repository or project \\nlevel is exploited. In particular, contributions \\nfrom contributors with no location information \\nto projects created or owned by a known \\norganization are automatically assigned the \\norganization’s country (i.e., the country where \\nits headquarters are located). For example, \\ncontributions from a contributor with no \\nlocation information to an AI project owned by \\nMicrosoft will be assigned to the United States.\\nIf the above fails, a contributor’s location field is left \\nblank.As of October 2021, 71.2% of the contributions to \\npublic AI projects were mapped to a country using \\nthis methodology. However, a decreasing trend in \\nthe share of AI projects for which a location can be \\nidentified is observed in time, indicating a possible lag \\nin location reporting.\\nMeasuring Contributions to AI Projects\\nCollaboration on a given public AI project is measured \\nby the number of contributions—or “commits”—made \\nto it.\\nTo obtain a fractional count of contributions by \\ncountry, an AI project is divided equally by the total \\nnumber of contributions made to it. A country’s total \\ncontributions to AI projects is therefore given by the \\nsum of its contributions—in fractional counts—to each \\nAI project. In relative terms, the share of contributions \\nto public AI projects made by a given country is the \\nratio of that country’s contributions to each of the \\nAI projects in which it participates over the total \\ncontributions to AI projects from all countries.\\nIn future iterations, OECD.AI plans to include \\nadditional measures of contribution to AI software \\ndevelopment, such as issues raised, comments, and \\npull requests.\\nIdentifying Programming Languages and \\nDevelopment Tools Used in AI Projects\\nGitHub uses file extensions contained in a project to \\nautomatically tag it with one or more programming \\nlanguages and/or development tools. This implies that \\nmore than one programming language or development \\ntool could be used in a given AI project.Chapter 1: Research and DevelopmentAppendix'),\n",
       " Document(metadata={'source': 'materials/example.pdf', 'page': 54}, page_content='55\\nArtificial Intelligence\\nIndex Report 2023 Chapter 1 PreviewMeasuring the Quality of AI Projects\\nTwo quality measures are used to classify public AI \\nprojects:\\n •  Project impact: The impact of an AI project is \\ngiven by the number of managed copies (i.e., \\n“forks”) made of that project.\\n •  Project popularity: The impact of an AI project \\nis given by the number of followers (i.e., “stars”) \\nreceived by that project.\\nFiltering by project impact or popularity could help \\nidentify countries that contribute the most to high \\nquality projects.\\nMeasuring Collaboration\\nTwo countries are said to collaborate on a specific \\npublic AI software development project if there is \\nat least one contributor from each country with at \\nleast one contribution (i.e., “commit”) to the project. \\nDomestic collaboration occurs when two contributors \\nfrom the same country contribute to a project.Chapter 1: Research and DevelopmentAppendix')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It is not mentioned in the document which specific company released the multimodal model.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "chain = load_qa_chain(llm=OpenAI(), chain_type=\"map_reduce\")\n",
    "query = \"The multimodal model released by which company?\"\n",
    "chain.run(input_documents=documents, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document\n",
    "loader = PyPDFLoader(\"materials/example.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# select which embeddings we want to use\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# create the vectorstore\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# expose this index in a retriever interface\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "\n",
    "# create a chain to answer questions\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "query = \"What is the total number of publications?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the total number of publications?',\n",
       " 'result': ' The total number of publications is approximately 500,000 in 2021.',\n",
       " 'source_documents': [Document(metadata={'page': 4, 'source': 'materials/example.pdf'}, page_content='Chapter 1 Preview 5\\n496.01\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210100200300400500Number of AI Publications (in Thousands)Number of AI Publications in the World, 2010–21 \\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report Artificial Intelligence\\nIndex Report 2023\\nOverview\\nThe figures below capture the total number \\nof English-language and Chinese-language AI \\npublications globally from 2010 to 2021—by type, \\naffiliation, cross-country collaboration, and cross-\\nindustry collaboration. The section also breaks down 1.1 Publications\\npublication and citation data by region for AI journal \\narticles, conference papers, repositories, and patents.\\nTotal Number of AI Publications\\nFigure 1.1.1 shows the number of AI publications in \\nthe world. From 2010 to 2021, the total number of \\nAI publications more than doubled, growing from \\n200,000 in 2010 to almost 500,000 in 2021.\\n1 See the Appendix for more information on CSET’s methodology. For more on the challenge of defining AI and correctly capturing relevant bibliometric data, see the AI Index team’s \\ndiscussion in the paper “Measurement in AI Policy: Opportunities and Challenges.”This section draws on data from the Center for Security and Emerging Technology (CSET) at Georgetown University. CSET maintains a \\nmerged corpus of scholarly literature that includes Digital Science’s Dimensions, Clarivate’s Web of Science, Microsoft Academic Graph, \\nChina National Knowledge Infrastructure, arXiv, and Papers With Code. In that corpus, CSET applied a classifier to identify English-\\nlanguage publications related to the development or application of AI and ML since 2010. For this year’s report, CSET also used select \\nChinese AI keywords to identify Chinese-language AI papers; CSET did not deploy this method for previous iterations of the AI Index report.1\\nIn last year’s edition of the report, publication trends were reported up to the year 2021. However, given that there is a significant lag in the \\ncollection of publication metadata, and that in some cases it takes until the middle of any given year to fully capture the previous year’s \\npublications, in this year’s report, the AI Index team elected to examine publication trends only through 2021, which we, along with CSET, \\nare confident yields a more fully representative report.1.1 PublicationsChapter 1: Research and Development\\nFigure 1.1.1'),\n",
       "  Document(metadata={'page': 16, 'source': 'materials/example.pdf'}, page_content='Chapter 1 Preview 17\\nArtificial Intelligence\\nIndex Report 2023\\nAI Conference Publications\\nOverview\\nThe number of AI conference publications peaked in 2019, and fell 20.4% below the peak in 2021 (Figure 1.1.13). \\nThe total number of 2021 AI conference publications, 85,094, was marginally greater than the 2010 total of 75,592.1.1 PublicationsChapter 1: Research and Development\\n85.09\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021020406080100Number of AI Conference Publications (in Thousands)Number of AI Conference Publications, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.13')]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 4, 'source': 'materials/example.pdf'}, page_content='Chapter 1 Preview 5\\n496.01\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210100200300400500Number of AI Publications (in Thousands)Number of AI Publications in the World, 2010–21 \\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report Artificial Intelligence\\nIndex Report 2023\\nOverview\\nThe figures below capture the total number \\nof English-language and Chinese-language AI \\npublications globally from 2010 to 2021—by type, \\naffiliation, cross-country collaboration, and cross-\\nindustry collaboration. The section also breaks down 1.1 Publications\\npublication and citation data by region for AI journal \\narticles, conference papers, repositories, and patents.\\nTotal Number of AI Publications\\nFigure 1.1.1 shows the number of AI publications in \\nthe world. From 2010 to 2021, the total number of \\nAI publications more than doubled, growing from \\n200,000 in 2010 to almost 500,000 in 2021.\\n1 See the Appendix for more information on CSET’s methodology. For more on the challenge of defining AI and correctly capturing relevant bibliometric data, see the AI Index team’s \\ndiscussion in the paper “Measurement in AI Policy: Opportunities and Challenges.”This section draws on data from the Center for Security and Emerging Technology (CSET) at Georgetown University. CSET maintains a \\nmerged corpus of scholarly literature that includes Digital Science’s Dimensions, Clarivate’s Web of Science, Microsoft Academic Graph, \\nChina National Knowledge Infrastructure, arXiv, and Papers With Code. In that corpus, CSET applied a classifier to identify English-\\nlanguage publications related to the development or application of AI and ML since 2010. For this year’s report, CSET also used select \\nChinese AI keywords to identify Chinese-language AI papers; CSET did not deploy this method for previous iterations of the AI Index report.1\\nIn last year’s edition of the report, publication trends were reported up to the year 2021. However, given that there is a significant lag in the \\ncollection of publication metadata, and that in some cases it takes until the middle of any given year to fully capture the previous year’s \\npublications, in this year’s report, the AI Index team elected to examine publication trends only through 2021, which we, along with CSET, \\nare confident yields a more fully representative report.1.1 PublicationsChapter 1: Research and Development\\nFigure 1.1.1'),\n",
       " Document(metadata={'page': 16, 'source': 'materials/example.pdf'}, page_content='Chapter 1 Preview 17\\nArtificial Intelligence\\nIndex Report 2023\\nAI Conference Publications\\nOverview\\nThe number of AI conference publications peaked in 2019, and fell 20.4% below the peak in 2021 (Figure 1.1.13). \\nThe total number of 2021 AI conference publications, 85,094, was marginally greater than the 2010 total of 75,592.1.1 PublicationsChapter 1: Research and Development\\n85.09\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021020406080100Number of AI Conference Publications (in Thousands)Number of AI Conference Publications, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.13')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The total number of AI publications is almost 500,000 in 2021.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0),\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    vectorstore_cls=Chroma\n",
    ").from_loaders([loader])\n",
    "\n",
    "query = \"What is the total number of AI publications?\"\n",
    "index.query(llm=OpenAI(), question=query, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document\n",
    "loader = PyPDFLoader(\"materials/example.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# select which embeddings we want to use\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# create the vectorstore\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# expose this index in a retriever interface\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "\n",
    "# create a chain to answer questions\n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(), retriever)\n",
    "\n",
    "chat_history = []\n",
    "query = \"What is the total number of AI publications?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Almost 500,000 in 2021.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"What is this number divided by 2?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is the total number of AI publications?', ' Almost 500,000 in 2021.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No, I cannot divide the total number of AI publications by 2 as it is not provided in the given context.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
